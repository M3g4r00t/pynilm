{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 00:02:15.525415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-04 00:02:15.570519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-04 00:02:15.583838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 00:02:15.612840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 00:02:18.146295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set up device based on CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Chronos pipeline on specified device with bfloat16 precision\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\" if device.type == \"cpu\" else \"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(df: pd.DataFrame, chunk_size: int, stride: int = 1):\n",
    "    start = 0\n",
    "    length = df.shape[0]\n",
    "\n",
    "    # If DF is smaller than the chunk, return the DF\n",
    "    if length <= chunk_size:\n",
    "        return df[:]\n",
    "\n",
    "    # Producing individual chunks\n",
    "    dfs = []\n",
    "    # while start + chunk_size <= length:\n",
    "    #     dfs.append(df[start:chunk_size + start])\n",
    "    #     start = start + chunk_size\n",
    "    for i in range(0, length - chunk_size, stride):\n",
    "        dfs.append(df[i:i + chunk_size])\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_value = pd.read_csv('../data/redd_active_value_f1hz.csv')\n",
    "attributes = [c for c in df_value_value.columns.values if not c in ['timestamp']]\n",
    "labels = [c for c in df_value_value.columns.values if not c in ['timestamp', 'mains', 'amplitude_spectrum', 'phase_spectrum']]\n",
    "#labels = ['Fridge01','Dish washer01','Microwave01','Washer dryer01','Washer dryer02']\n",
    "predictors = ['mains']\n",
    "index_name = 'timestamp'\n",
    "training_start = '2011-04-16'\n",
    "training_end = '2011-05-16'\n",
    "test_start = '2011-05-17'\n",
    "test_end = '2011-05-31'\n",
    "\n",
    "# Ensure 'timestamp' column is in datetime format\n",
    "df_value_value[index_name] = pd.to_datetime(df_value_value[index_name])\n",
    "# Set the index as the timestamp\n",
    "df_value_value.set_index(index_name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size_opt = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Partition the DataFrame into training and test sets\n",
    "training_active_value_set = df_value_value.loc[training_start:training_end]\n",
    "training_active_value_set[predictors] = scaler.fit_transform(training_active_value_set[predictors])\n",
    "data_train = {}\n",
    "for a in labels: # type: ignore\n",
    "    p = predictors.copy()\n",
    "    p.append(a)\n",
    "    data_train[a] = chunkify(\n",
    "            training_active_value_set[p], # type: ignore\n",
    "            windows_size_opt,\n",
    "            windows_size_opt\n",
    "        )\n",
    "test_active_value_set = df_value_value.loc[test_start:test_end]\n",
    "test_active_value_set[predictors] = scaler.transform(test_active_value_set[predictors])\n",
    "data_test = {}\n",
    "for a in labels: # type: ignore\n",
    "    p = predictors.copy()\n",
    "    p.append(a)\n",
    "    data_test[a] = chunkify(\n",
    "            test_active_value_set[p], # type: ignore\n",
    "            windows_size_opt,\n",
    "            windows_size_opt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95504/95504 [12:39<00:00, 125.74it/s]   \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "result_path = '../data/v9'\n",
    "if not os.path.isdir(os.path.join(result_path)):\n",
    "    os.makedirs(os.path.join(result_path))\n",
    "\n",
    "# File to store the progress\n",
    "progress_file = os.path.join(result_path, 'progress.txt')\n",
    "indices_file = os.path.join(result_path, 'indices.txt')\n",
    "\n",
    "# Load existing progress if available\n",
    "saved_result = {p: [] for p in predictors}\n",
    "processed_indices = {p: set() for p in predictors}\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    # Load the processed indices\n",
    "    if os.path.exists(indices_file):\n",
    "        with open(indices_file, 'r') as f:\n",
    "            for line in f:\n",
    "                predictor, idx = line.strip().split(',')\n",
    "                processed_indices[predictor].add(int(idx))\n",
    "\n",
    "# Process the data\n",
    "for p in predictors:\n",
    "    result = saved_result[p]\n",
    "    for idx, ds in enumerate(tqdm(data_train[labels[0]])):\n",
    "        if idx in processed_indices[p]:\n",
    "            continue  # Skip already processed data\n",
    "\n",
    "        context = torch.tensor(np.array(ds[p]).tolist())\n",
    "        embeddings, tokenizer_state = pipeline.embed(context)\n",
    "\n",
    "        # Convert to float32\n",
    "        embedding_float32 = embeddings.to(torch.float32)\n",
    "\n",
    "        # Convert to float32\n",
    "        tokenizer_state_float32 = tokenizer_state.to(torch.float32)\n",
    "\n",
    "        # Flatten the embedding\n",
    "        flattened_embedding = torch.flatten(embedding_float32).numpy().tolist()\n",
    "\n",
    "        # Convert tokenizer state to a list (if it is not already flattened or if it’s a tensor)\n",
    "        tokenizer_state_list = tokenizer_state_float32.flatten().numpy().tolist()\n",
    "\n",
    "        # Concatenate the flattened embedding and the tokenizer state\n",
    "        combined_list = flattened_embedding + tokenizer_state_list\n",
    "\n",
    "        # Append the last row to the text file\n",
    "        with open(progress_file, 'a') as f:\n",
    "            f.write(','.join(map(str, combined_list)) + '\\n')\n",
    "\n",
    "        # Track processed index\n",
    "        with open(indices_file, 'a') as f:\n",
    "            f.write(f'{p},{idx}\\n')\n",
    "\n",
    "        processed_indices[p].add(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42728/42728 [13:55<00:00, 51.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# File to store the progress\n",
    "progress_file = os.path.join(result_path, 'progress2.txt')\n",
    "indices_file = os.path.join(result_path, 'indices2.txt')\n",
    "\n",
    "# Load existing progress if available\n",
    "saved_result2 = {p: [] for p in predictors}\n",
    "processed_indices2 = {p: set() for p in predictors}\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    # Load the processed indices\n",
    "    if os.path.exists(indices_file):\n",
    "        with open(indices_file, 'r') as f:\n",
    "            for line in f:\n",
    "                predictor, idx = line.strip().split(',')\n",
    "                processed_indices2[predictor].add(int(idx))\n",
    "\n",
    "# Process the data\n",
    "for p in predictors:\n",
    "    result = saved_result2[p]\n",
    "    for idx, ds in enumerate(tqdm(data_test[labels[0]])):\n",
    "        if idx in processed_indices2[p]:\n",
    "            continue  # Skip already processed data\n",
    "\n",
    "        context = torch.tensor(np.array(ds[p]).tolist())\n",
    "        embeddings, tokenizer_state = pipeline.embed(context)\n",
    "\n",
    "        # Convert to float32\n",
    "        embedding_float32 = embeddings.to(torch.float32)\n",
    "        \n",
    "        # Convert to float32\n",
    "        tokenizer_state_float32 = tokenizer_state.to(torch.float32)\n",
    "\n",
    "        # Flatten the embedding\n",
    "        flattened_embedding = torch.flatten(embedding_float32).numpy().tolist()\n",
    "\n",
    "        # Convert tokenizer state to a list (if it is not already flattened or if it’s a tensor)\n",
    "        tokenizer_state_list = tokenizer_state_float32.flatten().numpy().tolist()\n",
    "\n",
    "        # Concatenate the flattened embedding and the tokenizer state\n",
    "        combined_list = flattened_embedding + tokenizer_state_list\n",
    "\n",
    "        # Append the last row to the text file\n",
    "        with open(progress_file, 'a') as f:\n",
    "            f.write(','.join(map(str, combined_list)) + '\\n')\n",
    "\n",
    "        # Track processed index\n",
    "        with open(indices_file, 'a') as f:\n",
    "            f.write(f'{p},{idx}\\n')\n",
    "\n",
    "        processed_indices2[p].add(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
