{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificar que TensorFlow puede detectar la GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame\n",
    "df_train = pd.read_csv('../data/train_value_min_label_windows_28.csv')\n",
    "df_test = pd.read_csv('../data/test_value_min_label_windows_28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo preentrenado (usar VGG16 o ResNet50)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes\n",
    "def load_and_process_image(image_folder, img_name):\n",
    "    img_r_path = os.path.join(image_folder, f\"{img_name}_mains.png\")\n",
    "    img_g_path = os.path.join(image_folder, f\"{img_name}_amplitude_spectrum.png\")\n",
    "    img_b_path = os.path.join(image_folder, f\"{img_name}_phase_spectrum.png\")\n",
    "    \n",
    "    img_r = Image.open(img_r_path).convert('L')\n",
    "    img_g = Image.open(img_g_path).convert('L')\n",
    "    img_b = Image.open(img_b_path).convert('L')\n",
    "    \n",
    "    img_r = img_r.resize((224, 224))\n",
    "    img_g = img_g.resize((224, 224))\n",
    "    img_b = img_b.resize((224, 224))\n",
    "    \n",
    "    img_rgb = Image.merge(\"RGB\", (img_r, img_g, img_b))\n",
    "    img_array = img_to_array(img_rgb)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = vgg_preprocess(img_array)  # Normalizar las imágenes en el rango esperado por VGG16\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "num_images = len(df_train)\n",
    "feature_size = model.output_shape[1]*model.output_shape[2]*model.output_shape[3]\n",
    "\n",
    "# Create a memory-mapped file to store the features\n",
    "features_filename = '../data/v4/features_train.dat'\n",
    "features_memmap = np.memmap(features_filename, dtype='float32', mode='w+', shape=(num_images, feature_size))\n",
    "\n",
    "# Extract features and store in memory-mapped file\n",
    "batch_size = 32  # Adjust batch size according to your system's memory capacity\n",
    "for start in range(0, num_images, batch_size):\n",
    "    end = min(start + batch_size, num_images)\n",
    "    batch_img_names = df_train['row'][start:end]\n",
    "    batch_features = []\n",
    "\n",
    "    for img_name in batch_img_names:\n",
    "        img_array = load_and_process_image('../data/v4/window_size_28/train/all', img_name)\n",
    "        feature = model.predict(img_array)\n",
    "        batch_features.append(feature.flatten())\n",
    "\n",
    "    # Write batch features to memory-mapped file\n",
    "    features_memmap[start:end] = np.array(batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush changes to disk\n",
    "features_memmap.flush()\n",
    "\n",
    "# Load the memory-mapped file for further processing\n",
    "features_train = np.memmap(features_filename, dtype='float32', mode='r', shape=(num_images, feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "num_images = len(df_test)\n",
    "feature_size = model.output_shape[1]*model.output_shape[2]*model.output_shape[3]\n",
    "\n",
    "# Create a memory-mapped file to store the features\n",
    "features_filename = '../data/v4/features_test.dat'\n",
    "features_memmap = np.memmap(features_filename, dtype='float32', mode='w+', shape=(num_images, feature_size))\n",
    "\n",
    "# Extract features and store in memory-mapped file\n",
    "batch_size = 32  # Adjust batch size according to your system's memory capacity\n",
    "for start in range(0, num_images, batch_size):\n",
    "    end = min(start + batch_size, num_images)\n",
    "    batch_img_names = df_test['row'][start:end]\n",
    "    batch_features = []\n",
    "\n",
    "    for img_name in batch_img_names:\n",
    "        img_array = load_and_process_image('../data/v4/window_size_28/test/all', img_name)\n",
    "        feature = model.predict(img_array)\n",
    "        batch_features.append(feature.flatten())\n",
    "\n",
    "    # Write batch features to memory-mapped file\n",
    "    features_memmap[start:end] = np.array(batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush changes to disk\n",
    "features_memmap.flush()\n",
    "\n",
    "# Load the memory-mapped file for further processing\n",
    "features_test = np.memmap(features_filename, dtype='float32', mode='r', shape=(num_images, feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las columnas de etiquetas\n",
    "labels_train = df_train.drop(columns=['row'])\n",
    "labels_test = df_test.drop(columns=['row'])\n",
    "# Diccionario para almacenar los resultados\n",
    "appliances = labels_train.columns\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo para guardar los resultados\n",
    "results_file = '../data/v4/model_results_window_size_28_vgg.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados existentes\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para guardar los resultados\n",
    "def save_results(results):\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [38:17<12:07:34, 2297.58s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.9 GiB for an array with shape (95504, 25088) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DennysMallqui\\Documents\\GitHub\\Proyectos 2024\\pynilm\\src\\EDA-08b-REDD.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# MLP\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_train_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X_test_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m mlp \u001b[39m=\u001b[39m MLPClassifier(hidden_layer_sizes\u001b[39m=\u001b[39m(\u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m),  \u001b[39m# Configuración con dos capas ocultas\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                     max_iter\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     learning_rate_init\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                     alpha\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DennysMallqui/Documents/GitHub/Proyectos%202024/pynilm/src/EDA-08b-REDD.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DennysMallqui\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\DennysMallqui\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:809\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 809\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\DennysMallqui\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:929\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_seen_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39misnan(X)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    928\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_seen_ \u001b[39m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m    930\u001b[0m             X,\n\u001b[0;32m    931\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_,\n\u001b[0;32m    932\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_,\n\u001b[0;32m    933\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_samples_seen_,\n\u001b[0;32m    934\u001b[0m             sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# missing values)\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mptp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_seen_) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DennysMallqui\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages\\sklearn\\utils\\extmath.py:987\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m     T \u001b[39m=\u001b[39m new_sum \u001b[39m/\u001b[39m new_sample_count\n\u001b[1;32m--> 987\u001b[0m     temp \u001b[39m=\u001b[39m X \u001b[39m-\u001b[39;49m T\n\u001b[0;32m    988\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    989\u001b[0m         \u001b[39m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[39m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         correction \u001b[39m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m    992\u001b[0m             np\u001b[39m.\u001b[39mmatmul, sample_weight, np\u001b[39m.\u001b[39mwhere(X_nan_mask, \u001b[39m0\u001b[39m, temp)\n\u001b[0;32m    993\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.9 GiB for an array with shape (95504, 25088) and data type float64"
     ]
    }
   ],
   "source": [
    "# Entrenar y evaluar modelos para cada columna\n",
    "for a in tqdm(appliances):\n",
    "    if a in results:\n",
    "        print(f\"Results for column {a} already exist. Skipping training.\")\n",
    "        continue\n",
    "\n",
    "    y_train = labels_train[a]\n",
    "    y_test = labels_test[a]\n",
    "    X_train = features_train\n",
    "    X_test = features_test\n",
    "    \n",
    "    # MLP\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(128, 64),  # Configuración con dos capas ocultas\n",
    "                        max_iter=500,\n",
    "                        learning_rate_init=0.001,\n",
    "                        solver='adam',\n",
    "                        alpha=0.01,\n",
    "                        random_state=42)\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "    \n",
    "    # XGBoost\n",
    "    #xgb = XGBClassifier(random_state=42)\n",
    "    #xgb.fit(X_train, y_train)\n",
    "    #y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "    # Evaluación\n",
    "    metrics = {}\n",
    "    \n",
    "    # Función para manejar errores en AUC\n",
    "    def safe_roc_auc_score(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError as e:\n",
    "            #print(f\"ROC AUC score error: {e}\")\n",
    "            return np.nan  # Retorna NaN si ocurre un error\n",
    "        \n",
    "    metrics['MLP'] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_mlp),\n",
    "        'auc': safe_roc_auc_score(y_test, y_pred_mlp),\n",
    "        'f1_score': f1_score(y_test, y_pred_mlp),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_mlp).tolist()  # Convertir a lista para guardar en JSON\n",
    "    }\n",
    "    \n",
    "    #metrics['XGBoost'] = {\n",
    "    #    'accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    #    'auc': safe_roc_auc_score(y_test, y_pred_xgb),\n",
    "    #    'f1_score': f1_score(y_test, y_pred_xgb),\n",
    "    #    'confusion_matrix': confusion_matrix(y_test, y_pred_xgb).tolist()  # Convertir a lista para guardar en JSON\n",
    "    #}\n",
    "    \n",
    "    results[a] = metrics\n",
    "\n",
    "    # Guardar resultados después de cada columna\n",
    "    save_results(results) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Sockets01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9700\n",
      "    AUC: 0.5378\n",
      "    F1 Score: 0.0909\n",
      "    Confusion Matrix:\n",
      "[[1293, 20], [20, 2]]\n",
      "Column: Sockets02\n",
      "  Model: MLP\n",
      "    Accuracy: 1.0000\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1335]]\n",
      "Column: Light01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9895\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1321, 14], [0, 0]]\n",
      "Column: CE appliance01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9056\n",
      "    AUC: 0.9122\n",
      "    F1 Score: 0.9079\n",
      "    Confusion Matrix:\n",
      "[[588, 13], [113, 621]]\n",
      "Column: Fridge01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.8434\n",
      "    AUC: 0.8317\n",
      "    F1 Score: 0.7996\n",
      "    Confusion Matrix:\n",
      "[[709, 84], [125, 417]]\n",
      "Column: Waste disposal unit01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9903\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1322, 13], [0, 0]]\n",
      "Column: Dish washer01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9783\n",
      "    AUC: 0.4928\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1306, 19], [10, 0]]\n",
      "Column: Electric furnace01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9835\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1313, 22], [0, 0]]\n",
      "Column: Light02\n",
      "  Model: MLP\n",
      "    Accuracy: 0.7940\n",
      "    AUC: 0.6047\n",
      "    F1 Score: 0.3619\n",
      "    Confusion Matrix:\n",
      "[[982, 46], [229, 78]]\n",
      "Column: Sockets03\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9528\n",
      "    AUC: 0.5103\n",
      "    F1 Score: 0.0308\n",
      "    Confusion Matrix:\n",
      "[[1271, 46], [17, 1]]\n",
      "Column: Light03\n",
      "  Model: MLP\n",
      "    Accuracy: 1.0000\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1335]]\n",
      "Column: Microwave01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9730\n",
      "    AUC: 0.5454\n",
      "    F1 Score: 0.1000\n",
      "    Confusion Matrix:\n",
      "[[1297, 19], [17, 2]]\n",
      "Column: Light04\n",
      "  Model: MLP\n",
      "    Accuracy: 0.8225\n",
      "    AUC: 0.6470\n",
      "    F1 Score: 0.4476\n",
      "    Confusion Matrix:\n",
      "[[1002, 57], [180, 96]]\n",
      "Column: Smoke alarm01\n",
      "  Model: MLP\n",
      "    Accuracy: 1.0000\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1335]]\n",
      "Column: Light05\n",
      "  Model: MLP\n",
      "    Accuracy: 0.8532\n",
      "    AUC: 0.6317\n",
      "    F1 Score: 0.3875\n",
      "    Confusion Matrix:\n",
      "[[1077, 64], [132, 62]]\n",
      "Column: Unknown01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9723\n",
      "    AUC: 0.4932\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1298, 18], [19, 0]]\n",
      "Column: Sockets04\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9281\n",
      "    AUC: nan\n",
      "    F1 Score: 0.0000\n",
      "    Confusion Matrix:\n",
      "[[1239, 96], [0, 0]]\n",
      "Column: Sockets05\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9715\n",
      "    AUC: 0.6056\n",
      "    F1 Score: 0.2083\n",
      "    Confusion Matrix:\n",
      "[[1292, 21], [17, 5]]\n",
      "Column: Washer dryer01\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9723\n",
      "    AUC: 0.7200\n",
      "    F1 Score: 0.4478\n",
      "    Confusion Matrix:\n",
      "[[1283, 19], [18, 15]]\n",
      "Column: Washer dryer02\n",
      "  Model: MLP\n",
      "    Accuracy: 0.9663\n",
      "    AUC: 0.7040\n",
      "    F1 Score: 0.4304\n",
      "    Confusion Matrix:\n",
      "[[1273, 22], [23, 17]]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar resultados\n",
    "for column, metrics in results.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    for model_name, scores in metrics.items():\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"    Accuracy: {scores['accuracy']:.4f}\")\n",
    "        print(f\"    AUC: {scores['auc']:.4f}\")\n",
    "        print(f\"    F1 Score: {scores['f1_score']:.4f}\")\n",
    "        print(f\"    Confusion Matrix:\\n{scores['confusion_matrix']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
